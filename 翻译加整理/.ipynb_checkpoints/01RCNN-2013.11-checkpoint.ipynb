{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rich feature hierarchies for accurate object detection and semantic segmentation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "利用丰富的层次特征准确实现目标检测和语义分割\n",
    "\n",
    "hierarchies 层次、\n",
    "Learning Multi-Level Hierarchies 分层强化学习\n",
    "semantic segmentation 语义分割 \n",
    "语义分割 在图像领域，语义指的是图像的内容，对图片意思的理解，输入一个人骑着自行车，能够将人和自行车分割开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在过去几年 ，目标检测在规范数据集PASCAL VOC上的测量性能逐渐趋于稳定\n",
    "\n",
    "canonical 规范的\n",
    "plateaued 趋于稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "表现最佳的方法是复杂的集合系统，通常将多个低级图像特征与高级上下文相结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在本文中，我们提出了一种简单可扩展的目标检测算法，相比于之前在VOC2012上的最佳结果，平均精度提高了0.3，平均精度达到了53.3%。\n",
    "\n",
    "scalable 可扩展的\n",
    "mean average precision 平均精度\n",
    "voc 2012 是一个数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach combines two key insights (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们的方法结合了两个观点：一是：在候选区域上自下而上使用大型卷积神经网络,用来定位和分割物体。二是，当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。\n",
    "\n",
    "scarce 稀缺\n",
    "high-capacity 大容量\n",
    "region proposals 候选区域\n",
    "候选区域方法：\n",
    "1.滑窗法(Sliding Window)：输入图像->不同大小窗口+滑动间隔->分类器打分确定分数高的->非极大值抑制（过滤重复的）\n",
    "2.选择性搜索(Selective Search):elective Search 的前期工作就是利用Graph-Based Image Segmentation的分割算法\n",
    "有监督学习：有特征有标签，经过训练得到特征和标签之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "因此我们将cnn与候选区域结合，这种方法叫做R-CNN：Regions with CNN features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/rbg/rcnn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们列出了一些实验结果，可以深入了解网络学习，揭示图像丰富特征层次结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features matter. The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT [26] and HOG [7]. But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection [12], it is generally acknowledged that progress has been slow during 2010-2012, with small gains obtained by building ensemble systems and employing minor variants of successful methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "特征很重要。在过去几十年间，各种计算机视觉识别任务都是建立在SITFT和HOG的基础之上。但是我们关注一下PASCAL VOC这个经典的视觉识别任务，2010-2012年进展缓慢，取得的微小进步都是通过构建一些集成系统和采用一些成功方法的变种才达到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT and HOG are blockwise orientation histograms, a representation we could associate roughly with complex cells in V1, the first cortical area in the primate visual path-way. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SIFT和HOG是块方向直方图(blockwise orientation histograms)，一种类似大脑初级皮层V1层复杂细胞的表示方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we also know that recognition occurs several stages downstream, which suggests that there might be hierarchical, multi-stage processes for computing features that are even more informative for visual recognition."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "但我们知道识别发生在多个下游阶段，也就是说对于视觉识别来说，更有价值的信息，是层次化的，多个阶段的特征。\n",
    "\n",
    "hierarchical: 分层的\n",
    "multi-stage：多阶段的\n",
    "informative ：有价值的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fukushima’s “neocognitron” [16], a biologically-inspired hierarchical and shift-invariant model for pattern recognition, was an early attempt at just such a process."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fukushima的“neocognitron，一种受生物学启发用于模式识别的层次化、移动不变性模型，算是这方面最早的尝试。\n",
    "\n",
    "shift-invariant 平移不变性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neocognitron, however, lacked a supervised training al- gorithm. LeCun et al. [23] provided the missing algorithm by showing that stochastic gradient descent, via backprop-agation, can train convolutional neural networks (CNNs), a class of models that extend the neocognitron."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "然而neocognitron缺乏监督学习算法。Lecun等人的工作表明基于反向传播的随机梯度下降(SGD)对训练卷积神经网络（CNNs）非常有效，CNNs被认为是继承自neocognitron的一类模型。\n",
    "\n",
    "supervised 监督\n",
    "neocognitron 神经认知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs saw heavy use in the 1990s (e.g., [24]), but then fell out of fashion, particularly in computer vision, with the rise of support vector machines. In 2012, Krizhevsky et al. [22] rekindled interest in CNNs by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [9, 10]. Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on LeCun’s CNN (e.g., max(x,0) rectifying non-linearities and “dropout” regularization)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cnn十九世纪九十年代被广泛使用，但是随着支持向量机的兴起他随之过时。2012年，Krizhevsky等人通过在基于网络的大规模图像挑战大赛中的图像分类的优异表现重新引起人们对cnns的兴趣。他们的成功在于在120万的标签图像上使用了一个大型的CNN，并且对LeCUN的CNN进行了一些改造（比如ReLU和Dropout Regularization）。\n",
    "\n",
    "\n",
    "支持向量机  ?\n",
    "relu（一个激活函数）还一个常用的sigmoid\n",
    "Dropout 丢弃正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The significance of the ImageNet result was vigorously debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imagenet 结果的重要意义在2012年ILSVRC的研讨会上展开了激烈的讨论。最核心问题被归结为一下几点：CNN在imagenet的分类多大程度上能够推广到PASCAL VOC的目标检测的挑战上。\n",
    "\n",
    "vigorously 激烈的\n",
    "distilled 归纳总结，原义蒸馏净化\n",
    "extent 程度范围\n",
    "generalize 推广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We answer this question decisively by bridging the chasm between image classification and object detection. This paper is the first to show that a CNN can lead to dra-matically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.1Achieving this result required solving two problems: localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们通过把图像分类和目标检测联系起来来回答这个问题。本文首次证明，与基于简单的像hog的特征，CNN在PASCAL VOC 目标检测上展示更好的性能。实现这个结果需要解决两个问题：使用深度网络定位物体和在小规模的标注数据集上进行大型网络模型的训练。\n",
    "\n",
    "decisively 果断地\n",
    "bridging  造桥链接关联"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlike image classification, detection requires localizing (likely many) objects within an image. One approach frames localization as a regression problem. However, work from Szegedy et al. [31], concurrent with our own, indicates that this strategy may not fare well in practice (they report a mAP of 30.5% on VOC 2007 compared to the 58.5% achieved by our method). An alternative is to build a sliding-window detector. CNNs have been used in this way for at least two decades, typically on constrained object categories, such as faces [28, 33] and pedestrians [29]. In order to maintain high spatial resolution, these CNNs typically only have two convolutional and pooling layers. We also considered adopting a sliding-window approach. <font color=#FF0000>However, units high up in our network, which has five convolutional layers, have very large receptive fields (195 × 195 pixels) and strides (32×32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge.</font>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "不像图像分类一样，目标检测需要在图像中定位目标位置。一种方法是把帧定位作为一种回归问题。但Szegedy等人的工作显示这种策略在实践中表现并不是很好（在VOC2007上他们的mAP是30.5%，而我们的达到了58.5%）。另一种方法是构件一种滑动窗口探测器。通过这种方法使用CNNs至少已经有20年的时间了，通常用于一些特定的种类如人脸，行人等。为了获得较高的分辨率这些网络都有两个卷积层和两个池化层。我们也考虑采用窗口滑动的方法。然而，我们的网络有5个卷积层，其中交深层的网络输入图像中有非常大的感受野(195 × 195像素)和步长(32×32像素)，这使得采用滑动窗口的方法充满挑战。\n",
    "\n",
    "maintain 保证\n",
    "spatial resolution 空间分辨率\n",
    " constrained 受限于\n",
    " pedestrians 行人\n",
    "frame 帧 框架\n",
    "regression 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead, we solve the CNN localization problem by operating within the<font color=red >“recognition using regions”</font> paradigm, as argued for by Gu et al. in [18]. At test-time, our method generates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs. We use a simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region’s shape. Figure 1 presents an overview of our method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN features."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "相反，正如Gu所主张的那样，我们是通过操作“recognition using regions” 范式，解决cnn的定位问题。测试时，对这每张图片，产生了接近2000个与类别无关的region proposal，对每个CNN抽取了一个固定长度的特征向量，然后借助专门针对特定类别数据的线性SVM对每个区域进行分类。我们不管每一个候选区域的大小，使用仿射变换从每个候选区域获得一个固定长度的向量作为CNN的输入。图1展示了我们方法的概述，并突出显示了一些结果。由于我们的系统将区域提议与CNN相结合，我们将这种方法命名为R-CNN:带有CNN特征的区域。\n",
    "\n",
    "paradigm 范例，范式\n",
    "generates 生成\n",
    "category-independent 类别无关的\n",
    "extract 提取\n",
    "compute 计算，估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning (e.g., [29]). <font color=red>The second major contribution of this paper is to show that supervised pretraining on a large auxiliary dataset (ILSVRC), followed by domain-specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. </font>In our experiments, fine-tuning for detection improves mAP performance by 8 percentage points. After fine-tuning, our system achieves a mAP of 54% on VOC 2010 compared to 33% for the highly-tuned, HOGbased deformable part model (<font color=red>DPM</font>)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "检测面临的第二大挑战是，标记数据缺乏，可获得标记数据不足以训练的大型的cnn。传统方法多是采用无监督与训练，再进行有监督调优。本文的第二个核心贡献是在辅助数据集（ILSVRC）上进行有监督预训练，再在小数据集上针对特定问题进行调优。这是在训练数据稀少的情况下一个非常有效的训练大型卷积神经网络的方法。我们的实验中，针对检测的调优将mAP提高了8个百分点。调优后，我们的系统在VOC2010上达到了54%的mAP，远远超过高度优化的基于HOG的可变性部件模型（deformable part model，DPM）\n",
    "\n",
    "scarce 缺乏\n",
    "insufficient 不充足的\n",
    " unsupervised 无监督的\n",
    " fine-tuning 细调，微调\n",
    " contribution 贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our system is also quite efficient. The only class-specific computations are a reasonably small matrix-vector product and greedy <font color=red>non-maximum suppression</font>. This computational property follows from features that are shared across all categories and that are also two orders of magnitude lowerdimensional than previously used region features (cf. [32])."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们的系统也是非常高效的的。其中涉及的小型矩阵相乘和贪婪非极大值抑制都是特定类别计算。这些计算属性在所有的特征类别中共享，这比之前使用的区域特征少了两个数量级的维度。\n",
    "\n",
    "small matrix-vector product 小型向量矩阵相乘\n",
    "non-maximum suppression 非极大值抑制\n",
    "property 属性\n",
    "\n",
    "（1）非极大值抑制：实际上这是一个迭代的过程，第一步的非极大值抑制就是选取了某一个最大的得分，然后删除了他周边的几个框，第二次迭代的时候在剩下的框里面选取一个最大的，然后再删除它周围iou区域大于一定阈值的，这样不停的迭代下去就会得到所有想要找到的目标物体的区域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One advantage of HOG-like features is their simplicity: it’s easier to understand the information they carry (although [34] shows that our intuition can fail us). Can we gain insight into the representation learned by the CNN? Perhaps the densely connected layers, with more than 54 million parameters, are the key? They are not. We “lobotomized” the CNN and found that a surprisingly large proportion, 94%, of its parameters can be removed with only a moderate drop in detection accuracy. Instead, by probing units in the network we see that the convolutional layers learn a diverse set of rich features (Figure 3)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HOG-like特征的一个优点是简单性：能够很容易明白提取到的特征是什么(尽管[34]表明我们的直觉可能会让我们失望)，那我们能可视化出CNNC提取到的特征吗？全连接层有超过5千4百万的参数值，这是关键吗？这些都不是，我们将CNN切断，会发现，移除掉其中94%的参数，精度只会下降一点点。相反，通过网络中的探测单元我们可以看到卷积层学习了一组丰富的特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the failure modes of our approach is also critical for improving it, and so we report results from the detection analysis tool of Hoiem et al. [20]. As an immediate consequence of this analysis, we demonstrate that a simple bounding box regression method significantly reduces mislocalizations, which are the dominant error mode."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分析我们方法的失败案例，对于进一步提高很有帮助，所以我们借助Hoiem等人的定位分析工具做实验结果的报告和分析。分析结果，我们发发现主要的错误是因为mislocalization,而使用了bounding box regression之后，可以有效的降低这个错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before developing technical details, we note that because R-CNN operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, we also achieve state-of-the-art results on the PASCALVOCsegmentationtask, withanaveragesegmentation accuracy of 47.9% on the VOC 2011 test set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "介绍技术细节之前，我们提醒大家由于R-CNN是在推荐区域上进行操作，所以可以很自然地扩展到语义分割任务上。只要很小的改动，我们就在PASCAL VOC语义分割任务上达到了很有竞争力的结果，在VOC2011测试集上平均语义分割精度达到了47.9%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Object detection with R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. In this section, we present our design decisions for each module, describe their test-time usage, detail how their parameters are learned, and show results on PASCAL VOC 2010-12."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们的物体检测系统有三个模块构成。第一个，产生类别无关的候选区域。这些建议定义了我们检测器可用的候选区域集合；第二个是一个大型卷积神经网络，用于从每个区域抽取特定大小的特征向量；第三个是一个指定类别的线性SVM。本部分，将展示每个模块的设计，并介绍他们的测试阶段的用法，以及参数是如何学习的细节，最后给出在PASCAL VOC 2010-12和ILSVRC2013上的检测结果。\n",
    "\n",
    "modules 模块\n",
    "extract 提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1Module design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposals. A variety of recent papers offer methods for generating category-independent region proposals. Examples include: objectness [1], selective search [32], category-independent object proposals [11], constrained parametric min-cuts (CPMC) [5], multi-scale combinatorial grouping [3], and Cires ¸an et al. [6], who detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals. While R-CNN is agnostic to the particular region proposal method, we use selective search to enable a controlled comparison with prior detection work (e.g., [32, 35])."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "近来有很多研究都提出了产生类别无关区域推荐的方法。比如: objectness（物体性），selective search（选择性搜索），category-independent object proposals(类别无关物体推荐)，constrained parametric min-cuts（受限参最小剪切, CPMC)，multi-scal combinatorial grouping(多尺度联合分组)，以及Ciresan等人的方法，他们将CNN用在规律空间块裁剪上以检测有丝分裂细胞，也算是一种特殊的区域推荐类型。由于R-CNN对特定区域算法是不关心的，所以我们采用了选择性搜索以方便和前面的工作进行可控的比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction. We extract a 4096-dimensional feature vector from each region proposal using the Caffe [21] implementation of the CNN described by Krizhevsky et al. [22]. Features are computed by forward propagating a mean-subtracted 227 × 227 RGB image through five convolutional layers and two fully connected layers. We refer readers to [21, 22] for more network architecture details."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "特征提取。我们使用Krizhevsky等人所描述的CNN的一个Caffe实现版本对每个候选区域抽取一个4096维度的特征向量。把一个输入为277*277大小的图片，通过五个卷积层和两个全连接层进行前向传播,最终得到特征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227 × 227 pixel size). Of the many possible transformations of our arbitrary-shaped regions, we opt for the simplest. Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16). Figure 2 shows a random sampling of warped training regions. The supplementary material discusses alternatives to warping."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "为了计算候选区域的特征向量，我们必须首先转换图像数据和CNN兼容，架构中的CNNC只能接受固定大小：277*277。有许多区域变换的方法，我们选择其中一个最简单的。无论候选区域的大小或者宽高比，我们都通过仿射变换把图像变为指定大小。具体的，变形之前，我们现在候选框周围加上16的padding,再进行各向异性缩放。图二展现了仿射变换训练区域的随机抽样。补充材料讨论了仿射变换的替代方法。\n",
    "\n",
    "compatible 兼容\n",
    "supplementary 补充\n",
    "alternatives 可代替的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Test-time detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective search’s “fast mode” in all experiments). We warp each proposal and forward propagate it through the CNN in order to read off features from the desired layer. Then, for each class, we score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, we apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over-union (IoU) overlap with a higher scoring selected region larger than a learned threshold."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在测试阶段，我在测试图像上采用选择性搜索去提取2000个候选区域（在实验中我们使用选择性搜索中的快速搜索）。我们变形每一个候选区域，通过CNN的向前传播读取特征。然后，对于每个类我们使用该类训练过的SVM对每一个提取出的向量进行打分。在图像中所有区域都被给予分数，我们使用贪婪非极大值抑制，如果该区域与一个评分更高、大于学习阈值的选定区域存在交集-联合(IoU)重叠，则拒绝该区域。\n",
    "\n",
    "intersection-over-union 交际联合\n",
    "threshold. 阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run-time analysis. Two properties make detection efficient. First, all CNN parameters are shared across all categories. Second, the feature vectors computed by the CNN are low-dimensional when compared to other common approaches, such as spatial pyramids with bag-of-visual-word encodings. The features used in the UVA detection system [32], for example, are two orders of magnitude larger than ours (360k vs. 4k-dimensional)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "运行时分析。两个特性使检测十分有效。首先，所有的CNN参数使共享的。其次，相比其他常见的方法比如带有编码的空间金字塔，CNN计算出的特征向量维度是很低的。在UVA探测系统中是比我们高出两个数量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result of such sharing is that the time spent computing region proposals and features (13s/image on a GPU or 53s/image on a CPU) is amortized over all classes. The only class-specific computations are dot products between features and SVM weights and non-maximum suppression. In practice, all dot products for an image are batched into a single matrix-matrix product. The feature matrix is typically 2000×4096 and the SVM weight matrix is 4096×N, where N is the number of classes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这种共享的结果是花费在计算候选区域和特征的时间在所有类上分摊。唯一的和具体类别有关的计算是特征向量和SVM权重的点积，以及NMS。实践中，所有的点积都可以批量化成一个单独矩阵间运算。\n",
    "特征矩阵是典型的2000*4096，支持向量机的权重矩阵是4096*N，N是类别的数量。\n",
    "amortize 分摊\n",
    "batch 批量化处理\n",
    "matrix 矩阵 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This analysis shows that R-CNN can scale to thousands of object classes without resorting to approximate techniques, such as hashing. Even if there were 100k classes, the resulting matrix multiplication takes only 10 seconds on a modern multi-core CPU. This efficiency is not merely the result of using region proposals and shared features. The UVA system, due to its high-dimensional features, would be two orders of magnitude slower while requiring 134GB of memory just to store 100k linear predictors, compared to just 1.5GB for our lower-dimensional features."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分析表明R-CNN可以扩展到上千个类别，而不需要借用近似技术（如hashing）。即使有10万个类别在现代多核cpu运行矩阵乘法也只需要10s。这种高校不仅是使用候选区域和共享向量的结果。由于高维特征，UVA系统存储100k线性预测器需要134G，而我们低纬度特征只需要1.5G。\n",
    "\n",
    "scale 扩展\n",
    "resort 借助\n",
    "approximate 近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is also interesting to contrast R-CNN with the recent work from Dean et al. on scalable detection using DPMs and hashing [8]. They report a mAP of around 16% on VOC 2007 at a run-time of 5 minutes per image when introducing 10k distractor classes. With our approach, 10k detectors can run in about a minute on a CPU, and because no approximations are made mAP would remain at 59% (Section 3.2)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "有趣的是R-CCN和最近Dean等人使用DPMs和hashing做检测的工作相比，他们用了1万个干扰类， 每五分钟可以处理一张图片，在VOC2007上的mAP能达到16%。我们的方法1万个检测器由于没有做近似，可以在CPU上一分钟跑完，达到59%的mAP（3.2节）。\n",
    "\n",
    "contrast 对比\n",
    "distractor 干扰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised pre-training. We discriminatively pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level annotations (i.e., no bounding box labels). Pretraining was performed using the open source Caffe CNN library [21]. In brief, our CNN nearly matches the performance of Krizhevsky et al. [22], obtaining a top-1 error rate 2.2 percentage points higher on the ILSVRC 2012 validation set. This discrepancy is due to simplifications in the training process."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "监督预训练：我们在一个大型辅助数据集(ILSVRC 2012)上使用图像级注释(没有边界框标签)对CNN进行了有区别的预训练。使用开源的Caffe CNN库[21]进行预训练。总体来说，我们的CNN十分接近krizhevsky等人的网络的性能，在ILSVRC2012分类验证集在top-1错误率上比他们高2.2%。差异主要来自于训练过程的简化。\n",
    "\n",
    "auxiliary 辅助\n",
    "discriminatively 有区别的\n",
    "annotations 注释\n",
    "discrepancy 差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-specific fine-tuning. To adapt our CNN to the new task (detection) and the new domain (warped VOC windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals from VOC. Aside from replacing the CNN’s ImageNet-specific 1000-way classification layer with a randomly initialized 21-way classification layer (for the 20 VOC classes plus background), the CNN architecture is unchanged. We treat all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives. We start SGD at a learning rate of 0.001 (1/10th of the initial pre-training rate), which allows fine-tuning to make progress while not clobbering the initialization. In each SGD iteration, we uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128. We bias the sampling towards positive windows because they are extremely rare compared to background."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "特定领域的参数调优。为了让我们的CNN适应适应新的任务（检测任务）和新的领域（变形后的候选区域）我们只使用推荐区域对CNN进行随机梯度下降。我们将CNN专用的ImageNet100-way分类层替换为随机初始化的21-way分类层（其中20是VOC的类别数，1代表背景），CNN结构没有改变。我们处理所有候选区域，如果其与真实标注的框的iou>=0.5则认为真值，否则为假。我们使用随机梯度下降的学习率为0.001（初始化预训练的十分之一），这使得调优得以有效进行而不会破坏初始化的成果。在每轮SGD迭代过程中，我们统一使用32个正例窗口（跨所有类别）和96个背景窗口，即每个mini-batch的大小是128。另外我们倾向于采样正例窗口，因为和背景相比他们很稀少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object category classifiers. Consider training a binary classifier to detect cars. It’s clear that an image region tightly enclosing a car should be a positive example. Similarly, it’s clear that a background region, which has nothing to do with cars, should be a negative example. Less clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives. The overlap threshold, 0.3, was selected by a grid search over {0,0.1, . . . ,0.5} on a validation set. We found that selecting this threshold carefully is important. Setting it to 0.5, as in [32], decreased mAP by 5 points. Similarly, setting it to 0 decreased mAP by 4 points. Positive examples are defined simply to be the ground-truth bounding boxes for each class."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "目标种类分类器。考虑到一个检测汽车的而二分类器。一个图像区域紧紧封闭一个汽车是一个正例。同样的，一区域没有汽车只有背景就是反例。不明确的是如何标注一个只和车有部分重叠的区域。我们使用iou重叠阈值来解决这个问题，低于这个阈值被我们定于为负例。这个覆盖阈值我们选择0.3，是在验证集上基于{0, 0.1, … 0.5}通过网格搜索得到的。我们发现选择这个阈值很重要，设置这个阈值为0.5，MAP就会下降5个点，同样的设置为0，就会下降4个点。正例就是被定义为每个类实际的边框。\n",
    "\n",
    "classifier 二分类器\n",
    "enclosing 封闭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once features are extracted and training labels are applied, we optimize one linear SVM per class. Since the training data is too large to fit in memory, we adopt the standard <font color=red>hard negative mining method</font> [14, 30]. Hard negative mining converges quickly and in practice mAP stops increasing after only a single pass over all images."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一旦一个特征被提取出，训练标签被应用，我们优化每个线性SVM。由于训练数据过大不能装入内存，我们采用标准的高难负例挖掘方法，这个算法收敛和很快，将所有图片训练一次，MAP就停止增加了。\n",
    "\n",
    "converges 收敛\n",
    "hard negative mining method :我们先用初始样本集(即第一帧随机选择的正负样本)去训练网络，再用训练好的网络去预测负样本集中剩余的负样本，选择其中得分最高，即最容易被判断为正样本的负样本为困难样本，加入负样本集中，重新训练网络，循环往复，然后我们会发现:咦！我们的网络的分类性能越来越强了！假阳性负样本与正样本间也越来越相似了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In supplementary material we discuss why the positive and negative examples are defined differently in fine-tuning versus SVM training. We also discuss why it’s necessary to train detection classifiers rather than simply use outputs from the final layer (fc8) of the fine-tuned CNN."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们讨论了，为什么在fine-tunning和SVM训练这两个阶段，我们定义得正负样例是不同的。我们也会讨论为什么训练一个分类器是必要的，而不只是简单地使用来自调优后的CNN的最终fc8层的输出。\n",
    "\n",
    "fine-tunning阶段是由于CNN对小样本容易过拟合，需要大量训练数据，故对IoU限制宽松： IoU>0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适用于小样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Results on PASCAL VOC 2010-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the PASCAL VOC best practices [12], we validated all design decisions and hyperparameters on the VOC 2007 dataset (Section 3.2). For final results on the VOC 2010-12 datasets, we fine-tuned the CNN on VOC 2012 train and optimized our detection SVMs on VOC 2012 trainval. We submitted test results to the evaluation server only once for each of the two major algorithm variants (with and without bounding box regression)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "遵循PASCAL最佳实践步骤，我们在VOC2007的数据集上所有的设计思想和参数处理。在2010-2012的数据集上，我们针对其数据集微调了参数，对SVM在VOC2012数据集上进行了优化。针对两种主要的算法变体，我们只向服务器提交了一次结果。\n",
    "\n",
    "algorithm variants 算法变体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 shows complete results on VOC 2010. We compare our method against four strong baselines, including <font color=red>SegDPM </font>[15], which combines <font color=red>DPM</font> detectors with the output of a semantic segmentation system [4] and uses additional inter-detector context and image-classifier rescoring. The most germane comparison is to the <font color=red>UVA</font> system from Uijlings et al. [32], since our systems use the same region proposal algorithm. To classify regions, their method builds a four-level spatial pyramid and populates it with densely sampled SIFT, Extended <font color=red>OpponentSIFT</font> , and <font color=red>RGBSIFT</font>  descriptors, each vector quantized with 4000-word codebooks. Classification is performed with a histogram intersection kernel SVM. Compared to their multi-feature, non-linear kernel SVM approach, we achieve a large improvement in mAP, from 35.1% to 53.7% mAP, while also being much faster (Section 2.2). Our method achieves similar performance (53.3% mAP) on VOC 2011/12 test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "表一显示了我们在VOC2010上实现的结果。我们将我们的方法同四种基准算法做对比，其中包括SegDPM，这种方法将DPM检测子与语义分割系统相结合并且使用附加的inter-detector的环境和图片检测器。更加恰当的比较是同Uijling的UVA系统比较，因为我们的方法同样基于候选框算法。对于候选区域的分类，他们通过构建一个四层的金字塔，并且将之与SIFT，扩展的OpponentSIFT和RGB-SIFT描述子结合，每一个向量被量化为4000-word的codebook。分类任务由一个直方交叉核的SVM承担。与与他们的多特征、非线性核支持向量机方法相比，我们在mAP上取得了很大的改进，mAP从35.1%提高到53.7%，同时速度也快得多，从35.1%提升至53.7%，而且速度更快。我们的方法在VOC2011/2012测试集上达到了相似的检测效果mAP53.3%。\n",
    "\n",
    "germane 恰当的\n",
    "spatial pyramid 金字塔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization, ablation, and modes of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Visualizing learned features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-layer filters can be visualized directly and are easy to understand [22]. They capture oriented edges and opponent colors. Understanding the subsequent layers is more challenging. Zeiler and Fergus present a visually attractive deconvolutional approach in [36]. We propose a simple (and complementary) non-parametric method that directly shows what the network learned."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第一层过滤器可以直接可视化，很容易被理解。第一层可以捕获方向边缘和对立颜色。理解后面的层更具挑战性。Zeiler和Fergus列出了一种视觉上的反卷积方法。我们则使用了一种简单的非参数化方法，直接展示网络学到的东西。\n",
    "\n",
    "non-parametric 非参数\n",
    "subsequent 后面的，随后的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea is to single out a particular unit (feature) in the network and use it as if it were an object detector in its own right. That is, we compute the unit’s activations on a large set of held-out region proposals (about 10 million), sort the proposals from highest to lowest activation, perform nonmaximum suppression, and then display the top-scoring regions. Our method lets the selected unit “speak for itself” by showing exactly which inputs it fires on. We avoid averaging in order to see different visual modes and gain insight into the invariances computed by the unit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这个方法就是在网络中选出一特定的单元，把他当作正确类别的物体检测器来用。方法如下，我们计算出抽取出来的候选区域的激活值，按照激活值对候选区域进行排序，进行非极大值抑制，展示得分高的候选区域。我们让选定的区域自己说话，通过精确展示他触发的输入。我们避免平均化是为了看到不同的视觉模式和深入观察单元计算出来的不变性。\n",
    "\n",
    "single out 选出\n",
    "invariances 不变性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We visualize units from layer pool5, which is the maxpooled output of the network’s fifth and final convolutional layer. The pool5feature map is 6 × 6 × 256 = 9216dimensional. Ignoring boundary effects, each pool5unit has a receptive field of 195×195 pixels in the original 227×227 pixel input. A central pool5unit has a nearly global view, while one near the edge has a smaller, clipped support."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们可视化pool5，这是卷积网络的最后一层，也是卷积层的最大池化层输出。pool5的特征图是6 × 6 × 256 = 9216维。忽略掉边界影响，每个pool5层在原始输入为227*227时雨哦一个195*195的感受野。一个中央的pool5层有一个近乎全局的视野，而靠近边缘的有一个较小的，带裁剪的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each row in Figure 3 displays the top 16 activations for a pool5unit from a CNN that we fine-tuned on VOC 2007 trainval. Six of the 256 functionally unique units are visualized (the supplementary material includes more). These units were selected to show a representative sample of what the network learns. In the second row, we see a unit that fires on dog faces and dot arrays. The unit corresponding to the third row is a red blob detector. There are also detectors for human faces and more abstract patterns such as text and triangular structures with windows. The network appears to learn a representation that combines a small number of class-tuned features together with a distributed representation of shape, texture, color, and material properties. The subsequent fully connected layer fc6 has the ability to model a large set of compositions of these rich features."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "图3中每一行显示了我们在VOC2007实例上参数优化的CNN中pool5单元前16个激活。这里可视化了256个功能独特中的6个。这些样本被选出来展示网络所学到的内容。第二行，我们了解到一个单元晕倒狗的面部和斑点就会激活。第三行对应的单元是一个红色斑点的检测器这还有人面或者更抽象的部分（如文本和带三角结构的窗户的探测器）的检测器。这个网络似乎学到了一些类别调优相关的特征，这些特征都是形状、纹理、颜色和材质特性的分布式表示。而后续的fc6层则对这些丰富的特征建立大量的组合来表达各种不同的事物。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Ablation studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance layer-by-layer, without fine-tuning. To understand which layers are critical for detection performance, we analyzed results on the VOC 2007 dataset for each of the CNN’s last three layers. Layer pool5was briefly described in Section 3.1. The final two layers are summarized below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "没有调优,各层性能表现:为了理解那一层对目标检测至关重要,我们分析了CNN最后三层的每一层在VOC2007数据集上的结果.pool5在3.1部分被简要描述.最后的两个层将在下面总结."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer fc6is fully connected to pool5. To compute features, it multiplies a 4096×9216 weight matrix by the pool5 feature map (reshaped as a 9216-dimensional vector) and then adds a vector of biases. This intermediate vector is component-wise half-wave rectified (x ← max(0, x))."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "层fc6i时pool5的全连接层.为了计算特征,通过pool5的特征图(被变形为9216维度的向量),他乘了一个4096*9216的去权重矩阵,并添加了一个biases.中间向量是逐个组件的半波整流."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer fc7is the final layer of the network. It is implemented by multiplying the features computed by fc6by a 4096 × 4096 weight matrix, and similarly adding a vector of biases and applying half-wave rectification."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fc7是网络的最后一个层,跟fc6之间通过一个4096×4096的矩阵相乘。也是添加了bias向量和应用了ReLU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by looking at results from the CNN without fine-tuning on PASCAL, i.e. all CNN parameters were pretrained on ILSVRC 2012 only. Analyzing performance layer-by-layer (Table 2 rows 1-3) reveals that features from fc7generalize worse than features from fc6. This means that 29%, or about 16.8 million, of the CNN’s parameters can be removed without degrading mAP . More surprising is that removing both fc7and fc6 produces quite good results even though pool5 features are computed using only 6% of the CNN’s parameters. Much of the CNN’s representational power comes from its convolutional layers, rather than from the much larger densely connected layers. This finding suggests potential utility in computing a dense feature map, in the sense of HOG, of an arbitrary-sized image by using only the convolutional layers of the CNN. This representation would enable experimentation with sliding-window detectors, including DPM, on top of pool5features."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们先看看没有调优的CNN在PASCAL上的表现,即所有CNN参数是被预训练在 ILSVRC 2012上.分析每一层性能显示来自fc7的特征泛化能力不如fc6的特征.这就意味着29%的CNN参数也就是1680万的参数可以去掉,而且不会影响MAP.即使去掉fc6和fc7也会产生很好的结果，即使pool5特性及使用6%的CNN参数及进行计算。多数CNN的主要表达力来自于卷积层，而不是全连接层。仅使用CNN的卷积层，在计算任意大小的图像的密集特征图(HOG)方面具有潜在的效用。这种表示可以直接在pool5的特征上进行滑动窗口检测的实验。\n",
    "\n",
    "\n",
    "densely connected layers 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance layer-by-layer, with fine-tuning. We now look at results from our CNN after having fine-tuned its parameters on VOC 2007 trainval. The improvement is striking (Table 2 rows 4-6): fine-tuning increases mAP by 8.0 percentage points to 54.2%. The boost from fine-tuning is much larger for fc6and fc7than for pool5, which suggests that the pool5features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "调优各层表现。我现在在CNN上看到的结果是在对VOC2007数据集上进过调优的。提升非常明显，mAP提升了8个百分点，达到了54.2%。fc6和fc7的提升明显优于pool5，这说明pool5从ImageNet学习的特征通用性很强，在它之上层的大部分提升主要是在学习领域相关的非线性分类器。\n",
    "\n",
    "domain-specific 特定领域\n",
    "striking 突出明显的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to recent feature learning methods. Relatively few feature learning methods have been tried on PASCAL VOC detection. We look at two recent approaches that build on deformable part models. For reference, we also include results for the standard HOG-based DPM [17]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对比其他的特征学习算法。相当少的特征学习方法应用与VOC数据集。我们找到的两个最近的方法都是基于DPM。为了参照的需要，我们也将基于基本HOG的DFM方法的结果加入比较。\n",
    "\n",
    "HOG 梯度方向直方图 1.色彩归一会2.计算像素梯度3.及图像划分cell，计算描述符4.组合cell形成block，在计算描述符5.串联block描述符表示hog\n",
    "\n",
    "DPM：对于任意一张输入图像，提取其DPM特征图，然后将原始图像进行高斯金字塔上采样，然后提取其DPM特征图。对于原始图像的DPM特征图和训练好的Root filter做卷积操作，从而得到Root filter的响应图。对于2倍图像的DPM特征图，和训练好的Part filter做卷积操作，从而得到Part filter的响应图。然后对其精细高斯金字塔的下采样操作。这样Root filter的响应图和Part filter的响应图就具有相同的分辨率了。然后将其进行加权平均，得到最终的响应图。亮度越大表示响应值越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first DPM feature learning method, DPM ST [25], augments HOG features with histograms of “sketch token” probabilities. Intuitively, a sketch token is a tight distribution of contours passing through the center of an image patch. Sketch token probabilities are computed at each pixel by a random forest that was trained to classify 35×35 pixel patches into one of 150 sketch tokens or background."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "　第一个DPM的特征学习方法，DPM ST,它使用“sketch token”概率的直方图来增强HOG特征。直观的，一个略图就是通过图片中心轮廓的狭小分布。略图表征概率通过一个被训练出来的分类35*35像素路径为一个150略图表征的的随机森林方法计算。\n",
    " \n",
    " Intuitively 直观的\n",
    " tight 狭小的\n",
    " sketch tokens 缩略图\n",
    " DPM ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second method, DPM HSC [27], replaces HOG with histograms of sparse codes (HSC). To compute an HSC, sparse code activations are solved for at each pixel using a learned dictionary of 100 7 × 7 pixel (grayscale) atoms. The resulting activations are rectified in three ways (full and both half-waves), spatially pooled, unit ‘2normalized, and then power transformed (x ← sign(x)|x|α)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第二个方法，DPM HSC，将HOG特征替换成一个稀疏编码的直方图。为了计算HSC，在每个像素上使用一个学习到的1007 * 7 像素（灰度空间）原子求解稀疏码激活，）由此产生的激活以三种方式（全波和半波）整流，空间池化，l2标准化，然后进行幂运算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All R-CNN variants strongly outperform the three DPM baselines (Table 2 rows 8-10), including the two that use feature learning. Compared to the latest version of DPM, which uses only HOG features, our mAP is more than 20 percentage points higher: 54.2% vs. 33.7%—a 61% relative improvement. The combination of HOG and sketch tokens yields 2.5 mAP points over HOG alone, while HSC improves over HOG by 4 mAP points (when compared internally to their private DPM baselines—both use nonpublic implementations of DPM that underperform the open source version [17]). These methods achieve mAPs of 29.1% and 34.3%, respectively."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "所有的RCNN变种算法都要强于这三个 DPM方法（表2， 8-10行），包括两种特征学习的方法与最新版本的DPM方法比较，我们的mAP要多大约20个百分点，61%的相对提升。略图表征与HOG现结合的方法比单纯HOG的性能高出2.5%，而HSC的方法相对于HOG提升四个百分点（当内在的与他们自己的DPM基准比价，全都是用的非公共DPM执行，这低于开源版本）。这些方法分别达到了29.1%和34.3%。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Detection error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We applied the excellent detection analysis tool from Hoiem et al. [20] in order to reveal our method’s error modes, understand how fine-tuning changes them, and to see how our error types compare with DPM. A full summary of the analysis tool is beyond the scope of this paper and we encourage readers to consult [20] to understand some finer details (such as “normalized AP”). Since the analysis is best absorbed in the context of the associated plots, we present the discussion within the captions of Figure 4 and Figure 5. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
